{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wk2 Demo - Intro to Hadoop Streaming\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information`__\n",
    "\n",
    "Last week you implemented your first MapReduce Algorithm using a bash script framework. We saw that adding a sorting component to our framework allowed us to write a more efficient reducer script and perform word counting in parallel. In this notebook, we'll introduce a new framework: Hadoop Streaming. Like before, you'll write mapper and reducer scripts in python then pass them to the framework which will stream over your input files, split them into chunks and sort to your specification. Although Hadoop Streaming is rarely used in production anymore it is the precursor to systems like Spark and as such is a useful way to illustrate key concepts in parallel computation. By the end of this live session you should be able to:\n",
    "* __... describe__ the main components and default behavior of the Hadoop Streaming framework.\n",
    "* __... write__ a Hadoop MapReduce job from scratch.\n",
    "* __... access__ the Hadoop Streaming UI and use it in debugging your jobs.\n",
    "* __... design__ Hadoop MapReduce implementations for simple tasks like counting and ordering.\n",
    "* __... explain__ why sorting with multiple reducers requires some extra work (as opposed to sorting with a single reducer).\n",
    "\n",
    "**Note**: Hadoop Streaming syntax is very particular. Make sure to test your python scripts before passing them to the Hadoop job and pay careful attention to the order in which Hadoop job parameters are specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloads\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For convenience, let's set a few global variables for paths you'll use frequently. __`NOTE:`__ _you may need to modify the jar file and HDFS (or local home) directory paths to match your environment. The paths below should work on the course Docker image. Refer to_ [this debugging FAQ](https://github.com/UCB-w261/main/blob/master/HelpfulResources/Hadoop/debugging-hadoop.md) _if you are unsure of the correct paths or encounter errors._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to java archive files for the hadoop streaming application on your machine\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/root/demo2': File exists\n"
     ]
    }
   ],
   "source": [
    "# hdfs directory where  we'll store files for this assignment\n",
    "HDFS_DIR = \"/user/root/demo2\"\n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local directory where you've cloned the course repo \n",
    "# in docker this is the path where your clone is mounted -- ADJUST AS NEEDED\n",
    "HOME_DIR = \"/media/notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <--- SOLUTION --->\n",
    "# ... for instructors ...\n",
    "HOME_DIR = \"/media/notebooks/Instructors\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store notebook environment path\n",
    "from os import environ\n",
    "PATH  = environ['PATH']\n",
    "# NOTE: we can pass this variable to our Hadoop Jobs using -cmdenv PATH={PATH}\n",
    "# This will ensure that, among other things, Hadoop uses the right python version.\n",
    "# You should not *need* this until the very last question in part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "In this notebook, we'll continue working with the  _Alice in Wonderland_ text file from HW1 and the test file we created for debugging. Run the following cell to confirm that you have access to these files and save their location to a global variable to use in your Hadoop Streaming jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data subfolder - RUN THIS CELL AS IS\n",
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  169k  100  169k    0     0   473k      0 --:--:-- --:--:-- --:--:--  473k\n"
     ]
    }
   ],
   "source": [
    "# (Re)Download Alice Full text from Project Gutenberg - RUN THIS CELL AS IS \n",
    "# NOTE: feel free to replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "!curl \"http://www.gutenberg.org/files/11/11-0.txt\" -o data/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/alice_test.txt\n",
    "This is a small test file. This file is for a test.\n",
    "This small test file has two small lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the paths - RUN THIS CELL AS IS (if Option 1 failed)\n",
    "ALICE_TXT = PWD + \"/data/alice.txt\"\n",
    "TEST_TXT = PWD + \"/data/alice_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### alice.txt #########\n",
      "﻿Project Gutenberg’s Alice’s Adventures in Wonderland, by Lewis Carroll\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "######### alice_test.txt #########\n",
      "This is a small test file. This file is for a test.\n",
      "This small test file has two small lines.\n"
     ]
    }
   ],
   "source": [
    "# confirm the files are there - RUN THIS CELL AS IS\n",
    "!echo \"######### alice.txt #########\"\n",
    "!head -n 6 {ALICE_TXT}\n",
    "!echo \"######### alice_test.txt #########\"\n",
    "!cat {TEST_TXT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Overview: MapReduce Programming Paradigm\n",
    "The week 2 reading from _Data Intensive Text Processing with Map Reduce_ by Lin and Dyer gave a high level overview of the key issues faced by parallel computation frameworks. It also introduced the Hadoop MapReduce framework. Lets start by briefly reviewing some of the key concepts from this week's async:\n",
    "\n",
    "> __DISCUSSION QUESTIONS:__  \n",
    ">* What is MapReduce? How does it differ from Hadoop?  \n",
    "* What are the main ideas of the functional programming paradigm and how does MapReduce exemplify these ideas?\n",
    "* What is the basic data structure used in Hadoop MapReduce?\n",
    "* What does 'data/code co-location' mean? How does this principle contribute to the efficiency of a distributed computation?\n",
    "* What is a race condition in the context of parallel computation? Give an example.\n",
    "* What kind of _synchronization_ does Hadoop MapReduce perform by default? \n",
    "* What aspect of the synchronization process is computationally costly?\n",
    "* What is  Hadoop MapReduce's default sorting behavior? \n",
    "* Throughout this course we'll emphasize the goal of writing 'stateless' implementations? What does that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS__\n",
    "* What is MapReduce? How does it differ from Hadoop?\n",
    "> MapReduce is a programming model for processing large data sets with a parallel, distributed algorithm on a cluster. It has two phases: 'map' which applies a transformation(or filter) to all elements of a collection, and 'reduce' which aggregates/folds the elements of a collection. The term \"MapReduce\" is also commonly used to refer to an execution framework built around this paradigm. Hadoop MapReduce is one such popular open-source framework.  \n",
    "\n",
    "* What are the main ideas of the functional programming paradigm and how does MapReduce exemplify these ideas?\n",
    "> Hadoop MapReduce is based on the functional programing paradigm. It is a declarative programming paradigm, which means programming is done with expressions. In functional code, the output value of a function depends only on the arguments that are input to the function. Key ideas in this paradigm include avoiding state changes/mutable data, and using higher-order functions (functions which take other functions as inputs. Map and reduce are examples of higher-order functions.  \n",
    "\n",
    "* What is the basic data structure used in Hadoop MapReduce?\n",
    ">Hadoop MapReduce processes data in key-value pairs.  \n",
    "\n",
    "\n",
    "* What does 'data/code co-location' mean? How does this principle contribute to the efficiency of a distributed computation?\n",
    "> Data/code \"colocation\" is a principle in distributed processing by which we send the code to the node where a chunk of data is stored and perform the computation on that node. This is efficient because it limits network traffic.  \n",
    "\n",
    "* What is a race condition in the context of parallel computation? Give an example.\n",
    "> A race condition happens when multiple execution entities (e.g. threads, processes, etc) are accessing or modifying the same resource at the same time, and the order of access can impact the result. For example, suppose two concurrent threads or processes A and B attempt to increment the same memory location. Without proper synchronization, chances are that the following sequence will occur: A reads, B reads, A writes, B writes - with the final value being incremented only once. To overcome this conditon, the global/count variable must be locked during the update; otherwise a thread can read the count while other threads are updating it, and the final accumulation will be less than the true value.\n",
    "\n",
    "* What kind of _synchronization_ does Hadoop MapReduce perform by default? As a result of this synchronization, what is  Hadoop MapReduce's default sorting behavior? What aspect of the synchronization process is computationally costly?\n",
    "> Hadoop MapReduce performs synchronization using a barrier between the map and reduce phases. The reduce phase will not start until all mappers are complete. Then Hadoop reorganizes (a.k.a. 'shuffles') the mapper output which will be input to the reduce phase which will receive records sorted by key. This 'shuffle' is computationally costly because of the sorting and network transfer involved.  \n",
    "\n",
    "* Throughout this course we'll emphasize the goal of writing 'stateless' implementations? What does that mean?\n",
    "> We don't want our implementations to depend on any shared mutable information. This is related to the idea of avoiding race conditions & the optimality of 'embarassingly parallel' implementations. Note that we also like to limit the use of memory given that we are likely using a cluster of cheap machines that may have limited space.... but that in-memory aggregation on individual nodes is not a violation of the principle of 'statelessness' (though it might seem like we are storing a 'state' in that scenario, the 'statefulness' that we want to avoid is any shared information that needs to be updated across multiple nodes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview: Hadoop Streaming Syntax\n",
    "A basic Hadoop MapReduce job consists of three components: a mapper script, a reducer script and a  line of code in which you pass these scripts to the Hadoop streaming application/framework. The mapper and reducer can be any executable that will read from `stdin` and write to `stdout`, including a bash executable like`/bin/cat` which simply passes the lines of your file unchanged, or python scripts like the onese you wrote in HW1. \n",
    "\n",
    "To run your hadoop streaming job, you'll need an HDFS filepath for the input data. We can use the following line of code to load a local file into HDFS:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/root/demo2/alice_test.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# put the alice test file into HDFS - RUN THIS CELL AS IS\n",
    "!hdfs dfs -put {TEST_TXT} {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`TIP:`__ _Recall that we set the global variable_ `HDFS_DIR` _above to point to a directory called_ `demo2` _inside_ `/user/root/`, _you confirm that we've successfully loaded the data into HDFS using the following line. You can learn more about this command and others like it by reading the [Hadoop File System Shell Guide](https://hadoop.apache.org/docs/r2.7.5/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup         94 2020-01-20 23:13 /user/root/demo2/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "# confirm the data was loaded - RUN THIS CELL AS IS\n",
    "!hdfs dfs -ls {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, with that little bit of preparation, we can now run the most basic Hadoop Streaming Job.\n",
    "\n",
    ">__`DISCUSSION QUESTIONS`__ (_before you run the next cell ..._)   \n",
    "* Read the 6 lines below, what do each of them tell the framework to do?\n",
    "* What to the forward slashes indicate?\n",
    "* What do you expect to happen when we run this cell? (what should the output be? will it be printed to the console?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo2/test-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# first hadoop streaming job - RUN THIS CELL AS IS\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/test-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo2/test-output': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob3087849514129709477.jar tmpDir=null\n",
      "20/01/20 23:23:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/20 23:23:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/20 23:24:01 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/01/20 23:24:02 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/20 23:24:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579546961611_0001\n",
      "20/01/20 23:24:04 INFO impl.YarnClientImpl: Submitted application application_1579546961611_0001\n",
      "20/01/20 23:24:04 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1579546961611_0001/\n",
      "20/01/20 23:24:04 INFO mapreduce.Job: Running job: job_1579546961611_0001\n",
      "20/01/20 23:24:17 INFO mapreduce.Job: Job job_1579546961611_0001 running in uber mode : false\n",
      "20/01/20 23:24:17 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/20 23:24:30 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/01/20 23:24:31 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/20 23:24:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/20 23:24:41 INFO mapreduce.Job: Job job_1579546961611_0001 completed successfully\n",
      "20/01/20 23:24:41 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=106\n",
      "\t\tFILE: Number of bytes written=443755\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=369\n",
      "\t\tHDFS: Number of bytes written=96\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22497\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6529\n",
      "\t\tTotal time spent by all map tasks (ms)=22497\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6529\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=22497\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6529\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=23036928\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6685696\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=2\n",
      "\t\tMap output bytes=96\n",
      "\t\tMap output materialized bytes=112\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=112\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=376\n",
      "\t\tCPU time spent (ms)=2350\n",
      "\t\tPhysical memory (bytes) snapshot=620396544\n",
      "\t\tVirtual memory (bytes) snapshot=3941965824\n",
      "\t\tTotal committed heap usage (bytes)=386080768\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=141\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=96\n",
      "20/01/20 23:24:41 INFO streaming.StreamJob: Output directory: /user/root/demo2/test-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r {HDFS_DIR}/test-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/alice_test.txt \\\n",
    "  -output {HDFS_DIR}/test-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that running the cell above doesn't actually show us the results. That's because the results get written directly to the output directory in HDFS. You can view the results using an `hdfs` command. (__`Note:`__ `test-output` _is an HDFS directory that was created by the 5th line in the last cell, we could have named it anything we wanted_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2020-01-20 23:24 /user/root/demo2/test-output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         96 2020-01-20 23:24 /user/root/demo2/test-output/part-00000\n"
     ]
    }
   ],
   "source": [
    "# view the contents of the result directory - RUN THIS CELL AS IS\n",
    "!hdfs dfs -ls {HDFS_DIR}/test-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a small test file. This file is for a test.\t\n",
      "This small test file has two small lines.\t\n"
     ]
    }
   ],
   "source": [
    "# view the results themselves - RUN THIS CELL AS IS\n",
    "!hdfs dfs -cat {HDFS_DIR}/test-output/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__`DISCUSSION QUESTIONS`__ (_after running the Hadoop Job._)   \n",
    "* Do the results match your expectations?\n",
    "* Scan the Hadoop Job logging, what information stands out to you?\n",
    "* When would it be a bad idea to print the full results of a job to the console? what could we do instead?\n",
    "* What goes wrong if you try re-running the Hadoop Streaming job? Discuss two potential solutions to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS__\n",
    "* When discussing results point out the /bin/cat again... these are the 'unity' mapper and reducer... they just spit out the lines they receive.\n",
    "\n",
    "* When discussing the loggings goal here is just to get students familiar with how this looks... they may notice information about time & memory usage, or the count of mappers and reducers. We'll get into the Hadoop UI explicilty a bit later but you may choose to point this out now.\n",
    "\n",
    "* When would it be a bad idea to print the full results of a job to the console? what could we do instead?\n",
    "> For large output this is a waste of space. For really large output this could cause your notebook to crash. HDFS has a head command.\n",
    "\n",
    "* What goes wrong if you try re-running the Hadoop Streaming job? Discuss two potential solutions to this problem.\n",
    "> Goal here is for students to start to be comfortable receiving an error message. In this case, the Hadoop Job won't run if the output directory you specify is not empty. Two solutions 1) rename the output directory when you re-run the command (this will duplicate results) OR better still 2) use a Hadoop command like the one below to empty the output directory each time you re-run a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo2/test-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# <--- SOLUTION --->\n",
    "# clear the output directory before re-running a job. Eg.\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/test-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (before breakout 1)__\n",
    "* Breakouts can be awkward, you should jump in as quickly as possible you'll have just 10min for this one.\n",
    "\n",
    "* Note: for a lot of these tasks you'll be reading & modifying the python scripts in folders inside the current directory... highly recommend Jupyter lab's split screen for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 1 Tasks:\n",
    "* __a) read scripts & docstrings:__ Read through **`WordCount/mapper.py`** and **`WordCount/reducer.py`** scripts and pay attention to the docstrings. Note that they (briefly) explain what the script does and the expected input/output record formats. [`HINT`: _docstrings are a way to record information to help your reader (future-self/collaborator/grader) quickly orient to a piece of code. They should describe_ __what__  (_not_ how) _is being done. For more information refer to the [PEP 8 Style Guide for Python](https://www.python.org/dev/peps/pep-0008/)_] The use of docstrings is recommended in all code that is written for this class.\n",
    "\n",
    "* __b) discuss:__ What are the 'keys' and what are the 'values' in this MapReduce job? What delimiter separates them when we write to standard output? How will you expect Hadoop to sort the records emitted by the mapper script? Why is this order important given how the reducer script is written?\n",
    "\n",
    "\n",
    "* __c) run provided code:__ Run the cells provided to make sure that your mapper and reducer scripts are executable, load the input files into HDFS, and clear the HDFS directory where the job will write its output. You will need to do these preparation steps for all future Hadoop MapReduce jobs.\n",
    "\n",
    "\n",
    "* __d) unit test:__ A good habit when writing Hadoop streaming jobs is to test your mappers and reducers locally before passing them to a Hadoop streaming command. An easy way to do this is to pipe in a small line of text. We've provided the unix code to do so and added a unix sort to mimic Hadoop's default sorting. Run these cells to confirm that our mapper and reducer work properly. (Observe how the reducer doesn't work without the sort).\n",
    "\n",
    "\n",
    "* __e) code:__ We've provided the code to run your Hadoop streaming command on the test file. Read through this command and be sure you understand each parameter that we're passing in, then run it and confirm that the output performs word counting correctly. Finally, modify the code provided to run the Hadoop MapReduce job on the _Alice and Wonderland_ text instead of the test file. Remember that the input path you pass to the Hadoop streaming command should be a location in HDFS not a local path. Take a look at the output and confirm you get the same count for 'alice' as in HW1. Food for thought: _does the sorting match what you expected?_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`part c`** Prep for Hadoop Streaming Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - make sure the mapper and reducer are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x WordCount/mapper.py\n",
    "!chmod a+x WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/demo2/alice_test.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# part c - load the input files into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal {TEST_TXT} {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal {ALICE_TXT} {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo2/wordcount-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part c - clear the output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/wordcount-output\n",
    "# NOTE: this directory won't exist unless you are re-running a job, that's fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`part d`** Unit test your scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t1\n",
      "foo\t1\n",
      "quux\t1\n",
      "labs\t1\n",
      "foo\t1\n",
      "bar\t1\n",
      "quux\t1\n"
     ]
    }
   ],
   "source": [
    "# part d - unit test mapper script\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t2\n",
      "quux\t1\n",
      "labs\t1\n",
      "foo\t1\n",
      "bar\t1\n",
      "quux\t1\n"
     ]
    }
   ],
   "source": [
    "# part d - unit test reducer script\n",
    "!echo -e \"foo\t1\\nfoo\t1\\nquux\t1\\nlabs\t1\\nfoo\t1\\nbar\t1\\nquux\t1\" | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t2\n",
      "quux\t1\n",
      "labs\t1\n",
      "foo\t1\n",
      "bar\t1\n",
      "quux\t1\n"
     ]
    }
   ],
   "source": [
    "# part d - systems text mapper and reducer together\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\t1\n",
      "foo\t3\n",
      "labs\t1\n",
      "quux\t2\n"
     ]
    }
   ],
   "source": [
    "# part d - systems text mapper and reducer together with sort (RUN THIS CELL AS IS)\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | sort -k1,1 | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`part e`** Hadoop streaming command. __`NOTE:`__ _don't forget to clear the output directory before re-running this cell (see part b above)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob1135044501660803566.jar tmpDir=null\n",
      "20/01/20 23:31:13 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/20 23:31:13 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/20 23:31:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/01/20 23:31:15 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/20 23:31:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579546961611_0002\n",
      "20/01/20 23:31:15 INFO impl.YarnClientImpl: Submitted application application_1579546961611_0002\n",
      "20/01/20 23:31:15 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1579546961611_0002/\n",
      "20/01/20 23:31:15 INFO mapreduce.Job: Running job: job_1579546961611_0002\n",
      "20/01/20 23:31:26 INFO mapreduce.Job: Job job_1579546961611_0002 running in uber mode : false\n",
      "20/01/20 23:31:26 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/20 23:31:40 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/01/20 23:31:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/20 23:31:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/20 23:31:49 INFO mapreduce.Job: Job job_1579546961611_0002 completed successfully\n",
      "20/01/20 23:31:49 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=177\n",
      "\t\tFILE: Number of bytes written=448814\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=369\n",
      "\t\tHDFS: Number of bytes written=64\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23126\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6118\n",
      "\t\tTotal time spent by all map tasks (ms)=23126\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6118\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=23126\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6118\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=23681024\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6264832\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=131\n",
      "\t\tMap output materialized bytes=183\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=183\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=390\n",
      "\t\tCPU time spent (ms)=2300\n",
      "\t\tPhysical memory (bytes) snapshot=618184704\n",
      "\t\tVirtual memory (bytes) snapshot=3942129664\n",
      "\t\tTotal committed heap usage (bytes)=386080768\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=141\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=64\n",
      "20/01/20 23:31:49 INFO streaming.StreamJob: Output directory: /user/root/demo2/wordcount-output\n"
     ]
    }
   ],
   "source": [
    "# part e - Hadoop streaming job (RUN THIS CELL AS IS FIRST, then make your modification)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files WordCount/reducer.py,WordCount/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/alice_test.txt \\\n",
    "  -output {HDFS_DIR}/wordcount-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7228534081182341589.jar tmpDir=null\n",
      "18/04/28 16:20:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/04/28 16:20:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/04/28 16:20:22 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/04/28 16:20:22 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/04/28 16:20:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1524930837451_0001\n",
      "18/04/28 16:20:23 INFO impl.YarnClientImpl: Submitted application application_1524930837451_0001\n",
      "18/04/28 16:20:23 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1524930837451_0001/\n",
      "18/04/28 16:20:23 INFO mapreduce.Job: Running job: job_1524930837451_0001\n",
      "18/04/28 16:20:35 INFO mapreduce.Job: Job job_1524930837451_0001 running in uber mode : false\n",
      "18/04/28 16:20:35 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/04/28 16:20:46 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/04/28 16:20:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/04/28 16:20:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/04/28 16:20:58 INFO mapreduce.Job: Job job_1524930837451_0001 completed successfully\n",
      "18/04/28 16:20:58 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=275132\n",
      "\t\tFILE: Number of bytes written=902211\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=177909\n",
      "\t\tHDFS: Number of bytes written=28506\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17601\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8389\n",
      "\t\tTotal time spent by all map tasks (ms)=17601\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8389\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17601\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8389\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18023424\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=8590336\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=30423\n",
      "\t\tMap output bytes=214280\n",
      "\t\tMap output materialized bytes=275138\n",
      "\t\tInput split bytes=218\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3009\n",
      "\t\tReduce shuffle bytes=275138\n",
      "\t\tReduce input records=30423\n",
      "\t\tReduce output records=3009\n",
      "\t\tSpilled Records=60846\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=139\n",
      "\t\tCPU time spent (ms)=8960\n",
      "\t\tPhysical memory (bytes) snapshot=732729344\n",
      "\t\tVirtual memory (bytes) snapshot=4083589120\n",
      "\t\tTotal committed heap usage (bytes)=575143936\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177691\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=28506\n",
      "18/04/28 16:20:58 INFO streaming.StreamJob: Output directory: /user/root/demo2/wordcount-output\n"
     ]
    }
   ],
   "source": [
    "# <--- SOLUTION --->\n",
    "# part e - Hadoop streaming job (RUN THIS CELL AS IS FIRST, then make your modification)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files WordCount/reducer.py,WordCount/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/alice.txt \\\n",
    "  -output {HDFS_DIR}/wordcount-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part e - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/wordcount-output/part-0000* > WordCount/results.txt\n",
    "# NOTE: we would never do this for a really large output file! \n",
    "# (but it's convenient for illustration in this assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t2\n",
      "file\t3\n",
      "for\t1\n",
      "has\t1\n",
      "is\t2\n",
      "lines\t1\n",
      "small\t3\n",
      "test\t3\n",
      "this\t3\n",
      "two\t1\n"
     ]
    }
   ],
   "source": [
    "# part e - view results (RUN THIS CELL AS IS)\n",
    "!head WordCount/results.txt\n",
    "# NOTE: these words and counts should match your results in HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part e - check 'alice' count (RUN THIS CELL AS IS after running the job on the full file)\n",
    "!grep 'alice' WordCount/results.txt\n",
    "# EXPECTED OUTPUT: 403"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__DISCUSSION QUESTIONS (after breakout 1)__\n",
    "* What are the 'keys' and what are the 'values' in this MapReduce job? What delimiter separates them when we write to standard output? How will you expect Hadoop to sort the records emitted by the mapper script? Why is this order important given how the reducer script is written?\n",
    "* What was the default sorting you saw? When do you think this sorting happens?\n",
    "* Why go to the trouble of writing the reducer this way? why not just use a dictionary to count the words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (after breakout 1)__\n",
    "* What are the 'keys' and what are the 'values' in this MapReduce job? What delimiter separates them when we write to standard output? How will you expect Hadoop to sort the records emitted by the mapper script? Why is this order important given how the reducer script is written?\n",
    "> The keys are words the values are counts. The default delimiter in Hadoop streaming is a tab, however other delimiters (eg. comma, backslash, etc) could be specified. Hadoop will sort the records alphabetically by key (in our case, the word). This is important because the reducer will stream over the input records and emit the sum of consecutive counts (that belong to the same word). If the records were not alphabetized then the reducer might emit the same word twice without fully adding up its occurences.\n",
    "\n",
    "* What was the default sorting you saw? When do you think this sorting happens?\n",
    "> Sort by key, this happens between mapper & reducer (reducer needs it in order to work properly!)\n",
    "\n",
    "* Why go to the trouble of writing the reducer this way? why not just use a dictionary to count the words?\n",
    "> This goes back to the lesson from HW1... if we can't guarantee how much memory will be available in the nodes in our cluster of cheap comodity hardware, we want to try storing as little information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 2: Uppercase and Lowercase Counts\n",
    "\n",
    "What if we didn't care about individual word counts but rather wanted to know how many of the words in the _Alice_ file are upper and lower case? We could retrieve this information easily using a Hadoop streaming job with the same reducer script as in Section 2 (i.e. __`WordCount/reducer.py`__) but a slightly different mapper. In this question you'll design and write your own Hadoop streaming job to do just this.\n",
    "\n",
    "> __ DISCUSSION QUESTION:__  \n",
    "> * What should the keys be for this task? [`HINT:` _we'll need a key for each thing we want to count._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (before breakout 2)__\n",
    "* What should the keys be for this task? \n",
    "> We only need two keys: 'upper' and 'lower' (or something equivalent).\n",
    "\n",
    "* NOTE: by 'upper case' we mean the first letter is uppercase, not the whole word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 2 Tasks:\n",
    "* __a) code:__ Complete the docstring and code in the __`UpperLower/mapper.py`__ to create a mapper that reads each input line, splits it into words and emits an appropriate key-value pair for each one. [`HINT:` _we're going to use this mapper in conjunction with the reducer from Breakout 1 so your key-value format should look very similar to the one in Breakout 1's mapper._]\n",
    "\n",
    "\n",
    "* __b) unit test:__ Run the provided cells to make your new mapper executable and test that it works as you expect.\n",
    "\n",
    "\n",
    "* __c) code:__ We've provided the start of a Hadoop streaming command. Fill in the missing parameters following the example provided in Section 2. Run your Hadoop job on the test file to confirm that it works. When you are happy with the results replace the test filepath with the real _Alice in Wonderland_ filepath and rerun the job. We'll compare results when we come back from breakouts.\n",
    "\n",
    "* __d) discuss:__ Like our bash script HW1, Hadoop automatically splits up your records to be processed in parallel on separate mapper and reducer \"nodes\" (called \"tasks\" by Hadoop). Judging from the jobs you've run so far, what are the default number of 'map tasks' and 'reduce tasks' that Hadoop uses? Does this framework allow us to directly control the number of mappers and reducers? [__`HINTS`__: _to answer the first part of this question, look at the \"Job Counters\" section in the logging from your Hadoop job; for the second part of this question refer back to Lin & Dyer p24 at the very bottom_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a - run this cell after completing your portion of the code\n",
    "!chmod a+x UpperLower/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper\t1\n",
      "lower\t1\n",
      "upper\t1\n",
      "upper\t1\n",
      "lower\t1\n",
      "lower\t1\n",
      "lower\t1\n"
     ]
    }
   ],
   "source": [
    "# part b - unit test your new mapper (RUN THIS CELL AS IS)\n",
    "!echo \"Foo foo Quux Labs foo bar quux\" | UpperLower/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower\t4\n",
      "upper\t3\n"
     ]
    }
   ],
   "source": [
    "# part b - systems test your new mapper with the reducer from question 2 (RUN THIS CELL AS IS)\n",
    "!echo \"Foo foo Quux Labs foo bar quux\" | UpperLower/mapper.py | sort -k1,1 | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo2/upperlower-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part c - clear output directory before (re)running your Hadoop Job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/upperlower-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part c - Hadoop streaming command (FILL IN MISSING ARGUMENTS)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files UpperLower/mapper.py,WordCount/reducer.py \\\n",
    "\n",
    "    \n",
    "    \n",
    "  -output {HDFS_DIR}/upperlower-output \\ \n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.0.jar] /tmp/streamjob1292613142280270984.jar tmpDir=null\n",
      "18/09/09 21:22:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/09/09 21:22:11 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/09/09 21:22:12 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/09/09 21:22:12 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/09/09 21:22:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1536438260836_0005\n",
      "18/09/09 21:22:13 INFO impl.YarnClientImpl: Submitted application application_1536438260836_0005\n",
      "18/09/09 21:22:13 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1536438260836_0005/\n",
      "18/09/09 21:22:13 INFO mapreduce.Job: Running job: job_1536438260836_0005\n",
      "18/09/09 21:22:21 INFO mapreduce.Job: Job job_1536438260836_0005 running in uber mode : false\n",
      "18/09/09 21:22:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/09/09 21:22:29 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/09/09 21:22:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/09/09 21:22:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/09/09 21:22:38 INFO mapreduce.Job: Job job_1536438260836_0005 completed successfully\n",
      "18/09/09 21:22:38 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=280856\n",
      "\t\tFILE: Number of bytes written=1009872\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=177909\n",
      "\t\tHDFS: Number of bytes written=23\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12841\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4488\n",
      "\t\tTotal time spent by all map tasks (ms)=12841\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4488\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12841\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4488\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13149184\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4595712\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=28085\n",
      "\t\tMap output bytes=224680\n",
      "\t\tMap output materialized bytes=280862\n",
      "\t\tInput split bytes=218\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=280862\n",
      "\t\tReduce input records=28085\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=56170\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=81\n",
      "\t\tCPU time spent (ms)=5670\n",
      "\t\tPhysical memory (bytes) snapshot=796409856\n",
      "\t\tVirtual memory (bytes) snapshot=4109348864\n",
      "\t\tTotal committed heap usage (bytes)=576716800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177691\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=23\n",
      "18/09/09 21:22:38 INFO streaming.StreamJob: Output directory: /user/root/demo2/upperlower-output\n"
     ]
    }
   ],
   "source": [
    "# <--- SOLUTION --->\n",
    "# part c - Hadoop streaming command (FILL IN MISSING ARGUMENTS)  \n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files UpperLower/mapper.py,WordCount/reducer.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/alice.txt \\\n",
    "  -output {HDFS_DIR}/upperlower-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower\t24912\n",
      "upper\t3173\n"
     ]
    }
   ],
   "source": [
    "# part c - results (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/upperlower-output/part-000* > UpperLower/results.txt\n",
    "!cat UpperLower/results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __DISCUSSION QUESTIONS (after breakout 2)__\n",
    "* How many uppercase/lowercase words did you find?\n",
    "* How many map tasks? how many reduce tasks? Where did you find this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (after breakout 2)__\n",
    "* How many uppercase/lowercase words did you find?\n",
    "> `lower\t24912`\n",
    "`upper\t3173`\n",
    "\n",
    "* How many map tasks? how many reduce tasks? Where did you find this information?\n",
    "> By default Hadoop uses (at least) 2 mappers and 1 reducer. We can specify the number of reducers it should use and suggest a number of mappers, but we can't force it to use the number of mappers we suggest. Hadoop will make a determination based on the data size and location (and block size in HDFS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 1: WordCount in Hadoop MapReduce\n",
    "For most of the Hadoop jobs you write this week, you will use python scripts similar to those you wrote in HW1 to serve as your mapper and reducer. Since the Hadoop Streaming framework implements the principle of data/code co-location, we'll need to provide the paths to these python scripts so that Hadoop can ship them to the nodes where the code will get run. We do this by adding an additional flag to the Hadoop streaming command: the `-files` parameter. This will be the first of a number of additional flags that you can added to your Hadoop Streaming jobs to specify how the framework should handle your data. __`TIP:`__ _Hadoop can be very particular about the order in which you specify optional configuration fields. As a good debugging practice we recommend that you always maintain working code by starting with a basic job like the one we provided above and then testing the command as you add or modify parameters one by one_.\n",
    "\n",
    "For your first breakout activity we've provided an example of a Hadoop MapReduce job that performs word counting on an input file of your choice. The mapper and reducer are python scripts provided at __`WordCount/mapper.py`__ and __`WordCount/reducer.py`__. The Hadoop Streaming command is in a cell below. As you read through the example and go on to write your own Hadoop MapReduce jobs you may want to refer to Michael Noll's [blogpost on writing an MapReduce job](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/) and/or the [Hadoop Streaming documentation](https://hadoop.apache.org/docs/r2.7.5/hadoop-streaming/HadoopStreaming.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 3: Number of Unique Words\n",
    "\n",
    "Another variation on the simple word counting job would be to count the number of unique words in the book (instead of counting the unique occurrences of each word). Of course in reality the easiest way to get this information would be to count the number of lines in our word count output file... but since our goal here is to practice designing and writing Hadoop jobs, let's pretend for a moment that we don't have access to that file and instead think about how we'd do this from scratch. In this question we'll also introduce an important flag you can add to your Hadoop streaming jobs to control the degree of parallelization. \n",
    "\n",
    "> __DISCUSSION QUESTIONS:__  \n",
    "* What should our keys be for this task? \n",
    "* Does it make most sense to check for 'uniqueness' inside the mapper or inside the reducer? why? [`HINT:` _think about our discussion of memory constraints in HW1 and about the synchronization that Hadoop performs for us automatically between the map and reduce phases._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (before breakout 3)__\n",
    "* what should the keys/values be?\n",
    "> The keys should be words --> we need to sort these inorder to remove duplicates (& get the unique count)   \n",
    "\n",
    "* where should we check 'uniqueness'?\n",
    "> In the reducer after we've sorted (if we do it in mappers we could end up with duplicates).  Filtering for unique words inside the mapper would require maintaining a list of 'words seen' which could be memory intensive. Even if we decided to do this local aggregation, there could be duplicate keys emitted by map tasks on different nodes which means we can't fully filter out non-unique words in the map phase alone (unless we were to guarantee a single map task). Since the shuffle phase automatically groups records for each key together it will be easy and effcient to handle deduplication in the reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 3 Tasks:\n",
    "\n",
    "* __a) code + unit test:__ Since the mapper we wrote for `WordCount` already emits the right keys, let's simply reuse that mapper. Fill in the missing code in __`VocabSize/reducer.py`__so that this new reducer processes the records emitted by __`WordCount/mapper.py`__ and outputs the count of the number of unique words that appear in the input file. Run the provided unit test to confirm that your reducer works as you want it to.\n",
    "\n",
    "\n",
    "* __b) code:__ Write and run a Hadoop streaming job to calculate the number of unique words in _Alice and Wonderland_ and record it in the space provided (`NOTE:` _for 'c' you'll modify this job and overwrite the original result which is why we'll ask you to record it in markdown._)\n",
    "\n",
    "\n",
    "* __c) code + discussion:__ Add the flag `-numReduceTasks 3` to the very end of the Hadoop streaming command you wrote for `part c`. This flag tells Hadoop to use 3 separate reduce tasks, in other words, we'll make 3 'partitions' from the records emitted by your map phase and perform the reducing on each part. Rerun the job with this added flag and observe the result.  \n",
    "    * What do you notice about the contents of the HDFS output directory and the final output itself? How would we have to post process our results to get the answer we're looking for?\n",
    "\n",
    "\n",
    "* __d) Hadoop UI:__ In addition to the logging that Hadoop prints to your notebook you can also access two UIs with more detailed information about your Hadoop streaming jobs. While your job is currently running you can track its progress on port `8088` (this will be especially helpful in latter assignments when we run jobs that may take a long time).The link to this 'Running Job Tracker' UI can be found near the top of the logging from your job. Look for a line that reads something like:\n",
    ">`The url to track the job: http://quickstart.cloudera:8088/proxy/application_########_#####/`\n",
    "\n",
    " Once the job has completed, this link will redirect to the 'MapReduce Job History UI' on port `19888`. This is where you can access information about completed jobs (note the Job ID number will match the one printed in the URL above). \n",
    "  > `localhost:19888/jobhistory/job/job_########_#####`\n",
    "\n",
    " For `part d` Navigate to the MapReduce Job History UI (the one on port `19888`) and confirm that your job used 2 map tasks and 3 reduce tasks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part b - write your code in the provided script first, then RUN THIS CELL AS IS\n",
    "!chmod a+x VocabSize/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumUniqueWords\t4\n"
     ]
    }
   ],
   "source": [
    "# part b - unit test your new reducer (RUN THIS CELL AS IS)\n",
    "!echo -e \"foo\t1\\nfoo\t1\\nquux\t1\\nlabs\t1\\nfoo\t1\\nbar\t1\\nquux\t1\" | sort -k1,1 | VocabSize/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo2/vocabsize-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part c - clear output directory before (re)running your Hadoop Job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/vocabsize-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __`TIPS:`__ _When writing your job below make sure that you have the correct paths to your input file, output directory and mapper/reducer script. Don't forget the `-files` option, and_ DO NOT _put spaces between the paths that you pass to this option_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# parts b/c - write/modify your Hadoop streaming command here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/demo2/vocabsize-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob6076277652558050806.jar tmpDir=null\n",
      "19/05/18 00:44:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/18 00:44:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/05/18 00:44:27 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/05/18 00:44:27 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/05/18 00:44:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558046407799_0016\n",
      "19/05/18 00:44:28 INFO impl.YarnClientImpl: Submitted application application_1558046407799_0016\n",
      "19/05/18 00:44:28 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1558046407799_0016/\n",
      "19/05/18 00:44:28 INFO mapreduce.Job: Running job: job_1558046407799_0016\n",
      "19/05/18 00:44:37 INFO mapreduce.Job: Job job_1558046407799_0016 running in uber mode : false\n",
      "19/05/18 00:44:37 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/05/18 00:44:45 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/05/18 00:44:46 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/05/18 00:44:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/05/18 00:44:53 INFO mapreduce.Job: Job job_1558046407799_0016 completed successfully\n",
      "19/05/18 00:44:53 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=275132\n",
      "\t\tFILE: Number of bytes written=998763\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=177909\n",
      "\t\tHDFS: Number of bytes written=20\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10203\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4993\n",
      "\t\tTotal time spent by all map tasks (ms)=10203\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4993\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10203\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4993\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10447872\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5112832\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=30423\n",
      "\t\tMap output bytes=214280\n",
      "\t\tMap output materialized bytes=275138\n",
      "\t\tInput split bytes=218\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3009\n",
      "\t\tReduce shuffle bytes=275138\n",
      "\t\tReduce input records=30423\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=60846\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=87\n",
      "\t\tCPU time spent (ms)=4390\n",
      "\t\tPhysical memory (bytes) snapshot=760901632\n",
      "\t\tVirtual memory (bytes) snapshot=4123000832\n",
      "\t\tTotal committed heap usage (bytes)=754450432\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177691\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=20\n",
      "19/05/18 00:44:53 INFO streaming.StreamJob: Output directory: /user/root/demo2/vocabsize-output\n"
     ]
    }
   ],
   "source": [
    "# <--- SOLUTION --->\n",
    "# parts b/c - write/modify your Hadoop streaming command here:\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/vocabsize-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files WordCount/mapper.py,VocabSize/reducer.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/alice.txt \\\n",
    "  -output {HDFS_DIR}/vocabsize-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2019-05-18 00:44 /user/root/demo2/vocabsize-output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         20 2019-05-18 00:44 /user/root/demo2/vocabsize-output/part-00000\n"
     ]
    }
   ],
   "source": [
    "# parts b/c - take a look at the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -ls {HDFS_DIR}/vocabsize-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumUniqueWords\t3009\n"
     ]
    }
   ],
   "source": [
    "# parts b/c - view results (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/vocabsize-output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumUniqueWords\t1025\n",
    "NumUniqueWords\t1024\n",
    "NumUniqueWords\t960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __DISCUSSION QUESTIONS (after breakout 3)__\n",
    "* How many unique words were there?\n",
    "* What happened when 3 reduce tasks were specified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (after breakout 3)__\n",
    "\n",
    "*  How many unique words were there?\n",
    "> There are 3009 unique words in this text. (NOTE: the count of unique words is dependent on the tokenizer, this count assumes the provided tokenizer was used.)  \n",
    "\n",
    "*  What happened when 3 reduce tasks were specified?  \n",
    "> Specifying three reduce tasks causes our result file to include 3 sub-counts instead of one total count. We'd need to add these three numbers to get the true total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 4: Secondary Sort\n",
    "In breakout 1 we talked a little bit about the default sorting that Hadoop observes. However we'll often want to sort not just by the key but also by value. For example, we might want to sort the words by their count to find the most frequent words but then break ties by the word in alphabetical order. This is called a 'secondary sort'. In this question we'll learn about specifying parameters for sorting in Hadoop jobs. In particular you'll add three new parameters to your Hadoop Streaming command:  \n",
    "\n",
    "__`-D stream.num.map.output.key.fields=2`__ : tells Hadoop to treat both the first and the second (tab separated) fields as a composite key.  \n",
    "\n",
    "__`-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator`__ : tells Hadoop that we want to make comparisons (for sorting) based on the fields in this composite key\n",
    "\n",
    "__`-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\"`__: Tells Hadoop to perform a reverse numerical sort on the second field in the composite key and then break ties by sorting (alphabetically) on the first field in the composite key.\n",
    "\n",
    "To find the top words in the _Alice in Wonderland_ text we'll use the output of our word counting job as the input for this new sorting task. Recall that this output is a file in alphabetical order whose lines are of the format `word \\t count`. Also recall that this file is already available in HDFS at the path `{HDFS_DIR}/wordcount-output`. You can simply pass this directory path in to the Hadoop streaming input parameter and it will understand that it should read in the directory's contents. __`IMPORTANT`:__ _please use a single reduce task for parts a and b._\n",
    "\n",
    "> __DISCUSSION QUESTIONS (before breakout 4):__   \n",
    "* Before we get to the full secondary sort it's worth noting that there is a really easy way to get Hadoop to sort our file by count: we could just switch the order of the count and the word when we print to standard output in our mapper. Why does this work?\n",
    "* In the Hadoop job below we're using `/bin/cat` as our reducer... what is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (before breakout 4)__\n",
    "* Why would switching the order of the count & word achieve a secondary sort?\n",
    "> This would work because Hadoop sorts on the key by default, so if the key is the count then we should get a list of words by frequency. \n",
    "\n",
    "* In the Hadoop job below we're using `/bin/cat` as our reducer... what is that?\n",
    " > remember that we can use any executable as a mapper or reducer. This is the bash command (binary excecutable) that just reads/passses whatever we give it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 4 Tasks:\n",
    "\n",
    "* __a) code + discussion:__ Complete the code in __`TopWords/mapper.py`__ so that it performs the switch described above. For debugging purposes we'll first run this job using a test file of word counts instead of the full _Alice_ file. Run the provided Hadoop streaming command to confirm that our sneaky solution works. \n",
    "    * Notice that there is a problem with this result. Briefly discuss in groups what the problem is and why it happens.\n",
    "\n",
    "\n",
    "* __b) code:__ Ok, for obvious reasons our 'sneaky' solution didn't quite give us the output we wanted. So let's do a secondary sort properly this time. To do this, add the three new Hadoop options described in the intro to this question. Run your job with these new specifications on the dummy count file. When you are satisfied that your job works, change the input path to specify the alice count output that is already in HDFS. Your list of top words should match the result you got in HW1.\n",
    "    * __`Two important warnings:`__  1) Parameters starting with the `-D` flag must come immediately after the line where you specify the jar file and before the parameters `-files`, `-mapper`, etc; 2) The options we provided you above specify a reverse numeric sort on the second field and tie breaking using the first field... but the mapper you wrote in part a switched the order of the words and the counts. You will need to make a small adjustment to the options we provided so that it instead reverse numerically sorts by the first field and breaks ties on the second. \n",
    "\n",
    "\n",
    "* __c) code + discussion:__ Run your Hadoop job one more time but this time add the parameter to specify that the job should use 2 reduce tasks instead of 1. For convenience of illustration, you should do this using the sample counts file instead of the full _Alice_ text. \n",
    "    * Something is wrong with the results. Use the provided code to look at the output of each partition independently. Discuss why our results are off.  \n",
    "    * Imagine we had a really large file and performed a sort using 100 partitions, what post processing would we have to do to get a fully ordered list (Total Order Sort)? Compare the computational cost of this postprocessing to the postprocessing we discussed in the VocabSize job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The cell below will create a short file of word counts that we can load into HDFS and use to test our Hadoop MapReduce job. Take a moment to read this sample file and figure out what a reverse numerical sort (with alphabetical tie breaking) should yield. Then go on to complete your tasks as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TopWords/sample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile TopWords/sample.txt\n",
    "foo\t5\n",
    "quux\t9\n",
    "labs\t100\n",
    "bar\t5\n",
    "qi\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load sample file into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal TopWords/sample.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a - complete your work above then RUN THIS CELL AS IS\n",
    "!chmod a+x TopWords/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo2/topwords-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# parts a/b/c - clear output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/topwords-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a/b/c - Hadoop streaming command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files TopWords/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/topwords-output \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <--- SOLUTION --->\n",
    "# part a/b/c - Hadoop streaming command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1nr -k2,2\" \\\n",
    "  -files TopWords/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/topwords-output \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# part a/b/c - Save results locally (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/topwords-output/part-0000* > TopWords/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part a/b/c - view results (RUN THIS CELL AS IS)\n",
    "!head TopWords/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part c - look at first partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/topwords-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# part c - look at second partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/topwords-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected Results:__\n",
    "\n",
    "<table>\n",
    "<th>part a</th>\n",
    "<th>part b</th>\n",
    "<th>part c</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "1\tqi\n",
    "100\tlabs\n",
    "5\tfoo\n",
    "5\tbar\n",
    "9\tquux\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "100\tlabs\n",
    "9\tquux\n",
    "5\tbar\n",
    "5\tfoo\n",
    "1\tqi\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "9\tquux\n",
    "5\tbar\n",
    "100\tlabs\n",
    "5\tfoo\n",
    "1\tqi\n",
    "</pre></td>\n",
    "</tr></table>\n",
    "\n",
    ">__DISCUSSION QUESTIONS__:\n",
    "* What was the problem with the results in part a?\n",
    "* How do you know the secondary sort worked in part b?\n",
    "* What was the problem with the results in part c?\n",
    "* How could we post process these files (part c partitions) to produce a total order sort of them? Why would this be computationally expensive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--- SOLUTION --->\n",
    "__INSTRUCTOR TALKING POINTS (after breakout 4)__\n",
    "\n",
    "* What was the problem with the results in part a?\n",
    "> There is a big and a little problem here. The little problem is that we've sorted from smallest to largest when we'd probably have the opposite order if we're looking for a list of most frequent words. The bigger problem is that 'labs 100' isn't ordered correctly. It should be last but instead its second. This occurs because Hadoop is treating the counts as text values not as a numbers so they're sorted alphabetically not by value.\n",
    "\n",
    "* How do you know the secondary sort worked in part b?\n",
    "> there are two records with the same count (5) and the 'bar' comes before the 'foo'\n",
    "\n",
    "* What was wrong with the results in part c?\n",
    "> The results are not sorted properly because when we specify 2 reducers we end up with two sorted files concatenated together.  \n",
    "\n",
    "* How could we post-process these files to produce a total order sort of them? Why would this be computationally expensive?\n",
    "> To get a single list of words by frequency we'd need to merge sort these files (partitions) together. This is a much more computationally challenging task than simply adding the result of each mapper. In particular if we had lots of partitions (eg. 100) this mergesort would be O(100∗|V|)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout 5: Tracking Down Errors in Python Code\n",
    "You've now seen most of the basic functionality of writing and running Hadoop streaming jobs. In this week's homework and over the course of the next few weeks we'll explore additional options and tricks to add to our jobs. As we do this you will want to be able to quickly distinguish between errors that occur due to a mistake in your algorithm design or Hadoop streaming command and errors that are rooted in your Python code. Unfortunately the error logs printed to console do not always make this distinction obvious. Luckily, the Hadoop UI logs do make it very easy to identify Python coding errors. Before you move on to the homework (even if there isn't time in class) we'd like to make sure you know where to find these logs and how to fix two common mistakes. __Below, we provided code that contains two common errors for you to debug. Your job is to:__\n",
    "1. __Run the provided code as is, it will throw an error.__\n",
    "\n",
    "2. __Navigate to the Hadoop UI and find the relevant logs explaining your error.__ \n",
    " * Under `Task Type`, click `Map` > `task_XXXXXX` >`logs`\n",
    "\n",
    "3. __Fix the error(s) and re-run the job__. \n",
    "\n",
    "__`NOTE 1:`__ There are two different kinds of errors in the mapper code. See the inline comments for specific fixes: one involves adding a parameter to your Hadoop job the other two you must fix in the mapper code (re-run that cell to overwrite the old mapper). I'd recommend fixing them one at a time so that you can see how the logs and error messages change depending on the type of error.\n",
    "\n",
    "__`NOTE 2:` If you do not get to this section in class, you still must complete it before beginning the homework!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `demo': File exists\n"
     ]
    }
   ],
   "source": [
    "# run the following cells to create the demo mapper\n",
    "!mkdir demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell uses a little Jupyter Magic to create a python script on the fly, this is a useful technique that you may want to use in the future to create files for unit testing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This is a silly mapper to demonstrate some errors.\n",
    "\"\"\"\n",
    "import sys\n",
    "import numpy as np  # To use numpy add -cmdenv PATH={PATH} to your Hadoop Job\n",
    "\n",
    "for line in sys.stdin:\n",
    "    msg = (\"a message\")   # missing a parenthesis here\n",
    "    print(1/0)            # dividing by zero is a no-go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/demo2/demo-output\n"
     ]
    }
   ],
   "source": [
    "# clear HDFS output directory when you re-run the job\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/demo-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/demo2/demo-output': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob8586286894350858029.jar tmpDir=null\n",
      "19/09/07 18:37:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/09/07 18:37:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/09/07 18:37:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/09/07 18:37:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/09/07 18:37:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1567676064052_0018\n",
      "19/09/07 18:37:05 INFO impl.YarnClientImpl: Submitted application application_1567676064052_0018\n",
      "19/09/07 18:37:05 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1567676064052_0018/\n",
      "19/09/07 18:37:05 INFO mapreduce.Job: Running job: job_1567676064052_0018\n",
      "19/09/07 18:37:16 INFO mapreduce.Job: Job job_1567676064052_0018 running in uber mode : false\n",
      "19/09/07 18:37:16 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/09/07 18:37:23 INFO mapreduce.Job: Task Id : attempt_1567676064052_0018_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "19/09/07 18:37:24 INFO mapreduce.Job: Task Id : attempt_1567676064052_0018_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "19/09/07 18:37:32 INFO mapreduce.Job: Task Id : attempt_1567676064052_0018_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "19/09/07 18:37:32 INFO mapreduce.Job: Task Id : attempt_1567676064052_0018_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "19/09/07 18:37:40 INFO mapreduce.Job: Task Id : attempt_1567676064052_0018_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "19/09/07 18:37:41 INFO mapreduce.Job: Task Id : attempt_1567676064052_0018_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "19/09/07 18:37:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/09/07 18:37:51 INFO mapreduce.Job: Job job_1567676064052_0018 failed with state FAILED due to: Task failed task_1567676064052_0018_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "19/09/07 18:37:51 INFO mapreduce.Job: Counters: 14\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=7\n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tOther local map tasks=6\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=52428\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=52428\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=52428\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=53686272\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "19/09/07 18:37:51 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/demo-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files demo/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/demo-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <--- SOLUTION --->\n",
    "# Hadoop streaming command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files demo/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/demo-output \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-Up:  \n",
    "* The below images should be similar to what was found during the above exercise. Before beginning homework 2, make sure you can find these errors through the Hadoop UI. \n",
    "    * If you are unable to find these errors in the Hadoop UI logs, **seek the help of an instructor or TA immediately**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hadoop-module-error](HadoopModuleError.png)\n",
    "![hadoop-python-error](HadoopPythonError.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
