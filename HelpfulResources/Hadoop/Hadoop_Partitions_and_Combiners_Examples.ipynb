{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Streaming Partitions FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=TOC></a>\n",
    "## CONTENTS\n",
    "* [Sample file](#samp) \n",
    "* __Q__: _What partitioning does Hadoop do on its own?_ __A__: [Default Partitioning](#default) \n",
    "* __Q__: _How can I ask Hadoop to partition on a specific field?_ __A__: [Specifying a Parition Key](#specify)\n",
    "* __Q:__ _How can I sort within my custom partitions?_ __A:__ [Partitioning & Secondary Sort](#psort)\n",
    "* __Q:__ _Why doesn't my combiner recognize that the mapper output is sorted?_ __A:__ [part1](#combo1), [part2](#combo2)\n",
    "* __Q:__ _How can I make sure it does?_ __A:__ [Combining & Composite Keys](#combo3)\n",
    "* __Q:__ _Why does my combiner still not work fully?_ __A:__ [Default Combining](#combo-default)\n",
    "* __Q:__ _Why does a combiner mess up my secondary sort? - TRICK QUESTION_ __A:__ [Funky Stuff](#funky-stuff)\n",
    "* __Q:__ _What other funky things happen with combiners and partitions?_ __A:__ [More Funky Stuff](#more-funky-stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=samp></a>\n",
    "## Sample Input\n",
    "[Return to Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "A\tC\t1\n",
    "A\tC\t2\n",
    "A\tC\t1\n",
    "A\tD\t5\n",
    "A\tD\t5\n",
    "A\tD\t2\n",
    "B\tC\t5\n",
    "B\tC\t1\n",
    "B\tC\t10\n",
    "B\tD\t2\n",
    "B\tD\t10\n",
    "B\tD\t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `Data': File exists\n",
      "Deleted Data/test.txt\n"
     ]
    }
   ],
   "source": [
    "# put it into HDFS\n",
    "!hdfs dfs -mkdir Data\n",
    "!hdfs dfs -rm Data/test.txt\n",
    "!hdfs dfs -copyFromLocal test.txt Data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save my jar file for less typing\n",
    "JAR_FILE = '/usr/lib/hadoop-mapreduce/hadoop-streaming.jar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=default></a>\n",
    "## Default Partitioning\n",
    "[Return to Contents](#TOC)  | [Skip to Specifying a Non-Default Partition](#specify)   \n",
    "\n",
    "__Q__: _What partitioning does Hadoop do on its own (i.e. if we specify two reducers but don't tell it how to partition...)?_    \n",
    "__A:__ Hadoop will partition on the key. Though there is one important caveat -- read the warning at the end of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted no-partitioner-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob5795878492879064182.jar tmpDir=null\n",
      "17/09/22 23:43:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/22 23:43:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/22 23:43:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/22 23:43:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/22 23:43:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506091508167_0061\n",
      "17/09/22 23:43:30 INFO impl.YarnClientImpl: Submitted application application_1506091508167_0061\n",
      "17/09/22 23:43:30 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1506091508167_0061/\n",
      "17/09/22 23:43:30 INFO mapreduce.Job: Running job: job_1506091508167_0061\n",
      "17/09/22 23:43:38 INFO mapreduce.Job: Job job_1506091508167_0061 running in uber mode : false\n",
      "17/09/22 23:43:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/22 23:43:46 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/22 23:43:47 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/22 23:43:54 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/09/22 23:43:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/22 23:43:56 INFO mapreduce.Job: Job job_1506091508167_0061 completed successfully\n",
      "17/09/22 23:43:56 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=110\n",
      "\t\tFILE: Number of bytes written=575126\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=74\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13092\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11373\n",
      "\t\tTotal time spent by all map tasks (ms)=13092\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11373\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13092\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11373\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13406208\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11645952\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=74\n",
      "\t\tMap output materialized bytes=122\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=122\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=12\n",
      "\t\tSpilled Records=24\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=134\n",
      "\t\tCPU time spent (ms)=4540\n",
      "\t\tPhysical memory (bytes) snapshot=989495296\n",
      "\t\tVirtual memory (bytes) snapshot=5469573120\n",
      "\t\tTotal committed heap usage (bytes)=849346560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=110\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=74\n",
      "17/09/22 23:43:56 INFO streaming.StreamJob: Output directory: no-partitioner-output\n"
     ]
    }
   ],
   "source": [
    "# hadoop job w/ 2 reducers but no partitioner class\n",
    "!hdfs dfs -rm -r no-partitioner-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input Data/test.txt \\\n",
    "    -output no-partitioner-output \\\n",
    "    -numReduceTasks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tD\t2\n",
      "A\tD\t5\n",
      "A\tD\t5\n",
      "A\tC\t1\n",
      "A\tC\t2\n",
      "A\tC\t1\n"
     ]
    }
   ],
   "source": [
    "# first partition \n",
    "!hdfs dfs -cat no-partitioner-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tC\t5\n",
      "B\tD\t3\n",
      "B\tD\t10\n",
      "B\tD\t2\n",
      "B\tC\t10\n",
      "B\tC\t1\n"
     ]
    }
   ],
   "source": [
    "# second partition\n",
    "!hdfs dfs -cat no-partitioner-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WARNING:__ Hadoop, uses a hash function to perform this default partitioning. We got lucky and ended up with \"A\" in the first partition and \"B\" in the second but that is not guaranteed. With different keys they might end up in the opposite order or even both in the same partition! (_try replacing A with 'apple' and B with 'bear' to see this in action!_)\n",
    "\n",
    "__NOTE:__ Look closely, at the second partition output, there's something weird about the order of the records, this again has to do with Hadoop's default shuflle (Hint: _what sorting does Hadoop guarantee? in this example, what does Hadoop consider the 'key' to be?_). We'll return to this weirdness when we look at combiners below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=specify></a>\n",
    "## Specifying a Partition Key\n",
    "[Return to Contents](#TOC) | [Skip to Partition Sort](#psort)  \n",
    "\n",
    "__Q:__ _How can we ask Hadoop to partition on something other than the first field?_  \n",
    "__A__: Hadoop will only partition on a field that is part of the key so we must add 3 new paramters to the streaming command from the last section (note the order in which they appear below):  \n",
    "\n",
    ">__`-D stream.num.map.output.key.fields=2`__ _tells Hadoop to treat the first two fields form a composite key._    \n",
    ">__`-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner`__ _tells Hadoop that we want to partition based on one of the fields in the composite key._     \n",
    ">__`-D mapreduce.partition.keypartitioner.options=\"-k2,2\"`__  _tells Hadoop that in this example, we want to partition on the second field in the composite key._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted custom-partition-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob3184373659579329880.jar tmpDir=null\n",
      "17/09/22 23:49:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/22 23:49:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/22 23:49:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/22 23:49:40 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/22 23:49:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506091508167_0062\n",
      "17/09/22 23:49:41 INFO impl.YarnClientImpl: Submitted application application_1506091508167_0062\n",
      "17/09/22 23:49:41 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1506091508167_0062/\n",
      "17/09/22 23:49:41 INFO mapreduce.Job: Running job: job_1506091508167_0062\n",
      "17/09/22 23:49:49 INFO mapreduce.Job: Job job_1506091508167_0062 running in uber mode : false\n",
      "17/09/22 23:49:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/22 23:50:00 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/22 23:50:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/22 23:50:08 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/09/22 23:50:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/22 23:50:10 INFO mapreduce.Job: Job job_1506091508167_0062 completed successfully\n",
      "17/09/22 23:50:10 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=110\n",
      "\t\tFILE: Number of bytes written=577306\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=74\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16858\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11444\n",
      "\t\tTotal time spent by all map tasks (ms)=16858\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11444\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16858\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11444\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17262592\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11718656\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=74\n",
      "\t\tMap output materialized bytes=122\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=122\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=12\n",
      "\t\tSpilled Records=24\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=351\n",
      "\t\tCPU time spent (ms)=6520\n",
      "\t\tPhysical memory (bytes) snapshot=976527360\n",
      "\t\tVirtual memory (bytes) snapshot=5498093568\n",
      "\t\tTotal committed heap usage (bytes)=773849088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=110\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=74\n",
      "17/09/22 23:50:10 INFO streaming.StreamJob: Output directory: custom-partition-output\n"
     ]
    }
   ],
   "source": [
    "# same as above, but now we specify the partitioner & partition key\n",
    "!hdfs dfs -rm -r custom-partition-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k2,2\" \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input Data/test.txt \\\n",
    "    -output custom-partition-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tD\t2\n",
      "A\tD\t5\n",
      "A\tD\t5\n",
      "B\tD\t3\n",
      "B\tD\t10\n",
      "B\tD\t2\n"
     ]
    }
   ],
   "source": [
    "# first partition\n",
    "!hdfs dfs -cat custom-partition-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tC\t1\n",
      "A\tC\t2\n",
      "A\tC\t1\n",
      "B\tC\t5\n",
      "B\tC\t10\n",
      "B\tC\t1\n"
     ]
    }
   ],
   "source": [
    "# second partition\n",
    "!hdfs dfs -cat custom-partition-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! we partitioned based on the 2nd field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=psort></a>\n",
    "## Secondary Sort Partitions\n",
    "[Return to Contents](#TOC) | [Skip to sort with combiners](#combo1)\n",
    "\n",
    "__Q:__ _How can I sort within my custom partitions?_  \n",
    "__A:__ Since sorting is part of the shuffle and the shuffle is all about organizing by 'keys' we now want to include the third field as a part of our composite key: \n",
    "\n",
    ">__`-D stream.num.map.output.key.fields=3`__ _changed from '2' in the last example._    \n",
    "\n",
    "Then we add two more fields to specify unix style sorting on that field.\n",
    "\n",
    ">__`-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator`__ _tells Hadoop that we want to sort on one of the fields in our composite key._  \n",
    ">__`-D mapreduce.partition.keycomparator.options=\"-k3,3nr\"`__ _tells Hadoop that for this specific example we want a reverse numerical sort on the 3rd field._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted partition-sort-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob2474225473955301954.jar tmpDir=null\n",
      "17/09/22 23:50:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/22 23:50:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/22 23:50:25 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/22 23:50:25 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/22 23:50:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506091508167_0063\n",
      "17/09/22 23:50:25 INFO impl.YarnClientImpl: Submitted application application_1506091508167_0063\n",
      "17/09/22 23:50:25 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1506091508167_0063/\n",
      "17/09/22 23:50:25 INFO mapreduce.Job: Running job: job_1506091508167_0063\n",
      "17/09/22 23:50:34 INFO mapreduce.Job: Job job_1506091508167_0063 running in uber mode : false\n",
      "17/09/22 23:50:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/22 23:50:42 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/22 23:50:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/22 23:50:50 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/09/22 23:50:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/22 23:50:52 INFO mapreduce.Job: Job job_1506091508167_0063 completed successfully\n",
      "17/09/22 23:50:52 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=122\n",
      "\t\tFILE: Number of bytes written=578926\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=86\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11817\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11385\n",
      "\t\tTotal time spent by all map tasks (ms)=11817\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11385\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11817\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11385\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12100608\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11658240\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=86\n",
      "\t\tMap output materialized bytes=134\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=134\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=12\n",
      "\t\tSpilled Records=24\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=156\n",
      "\t\tCPU time spent (ms)=4610\n",
      "\t\tPhysical memory (bytes) snapshot=977428480\n",
      "\t\tVirtual memory (bytes) snapshot=5468557312\n",
      "\t\tTotal committed heap usage (bytes)=699924480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=110\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=86\n",
      "17/09/22 23:50:52 INFO streaming.StreamJob: Output directory: partition-sort-output\n"
     ]
    }
   ],
   "source": [
    "# same as above, but now we specify the partitioner & partition key\n",
    "!hdfs dfs -rm -r partition-sort-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k2,2\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input Data/test.txt \\\n",
    "    -output partition-sort-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tD\t10\t\n",
      "A\tD\t5\t\n",
      "A\tD\t5\t\n",
      "B\tD\t3\t\n",
      "B\tD\t2\t\n",
      "A\tD\t2\t\n"
     ]
    }
   ],
   "source": [
    "# first partition\n",
    "!hdfs dfs -cat partition-sort-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tC\t10\t\n",
      "B\tC\t5\t\n",
      "A\tC\t2\t\n",
      "A\tC\t1\t\n",
      "A\tC\t1\t\n",
      "B\tC\t1\t\n"
     ]
    }
   ],
   "source": [
    "# second partition\n",
    "!hdfs dfs -cat partition-sort-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't it make you happy when things are in order?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=combo1></a>\n",
    "## Default Combiners\n",
    "[Return to Contents](#TOC) | [Skip to part2 of this answer](#combo2)\n",
    "\n",
    "__Q:__ _Why doesn't my combiner recognize that the mapper output is sorted?_   \n",
    "__A (take 1):__ Chances are your mapper output __isn't actually sorted in the way you think__. Take a look at the following example where the combiner appears not to work properly, then keep reading for an explanation of why this is actually normal behavior & what do do if you want a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/opt/anaconda/bin/python\n",
    "\"\"\"\n",
    "A small combiner which sums the 3rd field.\n",
    "Input Format: group \\t part \\t integer\n",
    "NOTE: input must be pre-sorted by first 2 fields.\n",
    "\"\"\"\n",
    "import sys\n",
    "\n",
    "gp,part,num = ['','',None]\n",
    "for line in sys.stdin:\n",
    "    new_gp,new_part,new_num = line.split()    \n",
    "    # EITHER update current record\n",
    "    if new_gp == gp and new_part == part:\n",
    "        num += int(new_num) \n",
    "    # OR emit & update\n",
    "    else:\n",
    "        if num: # skips the initialized dummy count\n",
    "            print \"%s\\t%s\\t%s\"%(gp, part, num)\n",
    "        gp,part,num = new_gp,new_part,int(new_num)\n",
    "# emit the last record\n",
    "print \"%s\\t%s\\t%s\"%(gp,part,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `combiner-output': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob3074670169631509248.jar tmpDir=null\n",
      "17/09/25 19:49:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/25 19:49:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/25 19:49:26 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/25 19:49:26 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/25 19:49:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506368833436_0001\n",
      "17/09/25 19:49:28 INFO impl.YarnClientImpl: Submitted application application_1506368833436_0001\n",
      "17/09/25 19:49:28 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1506368833436_0001/\n",
      "17/09/25 19:49:28 INFO mapreduce.Job: Running job: job_1506368833436_0001\n",
      "17/09/25 19:49:41 INFO mapreduce.Job: Job job_1506368833436_0001 running in uber mode : false\n",
      "17/09/25 19:49:41 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/25 19:49:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/25 19:50:01 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/09/25 19:50:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/25 19:50:02 INFO mapreduce.Job: Job job_1506368833436_0001 completed successfully\n",
      "17/09/25 19:50:02 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=55\n",
      "\t\tFILE: Number of bytes written=581388\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=33\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16629\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11489\n",
      "\t\tTotal time spent by all map tasks (ms)=16629\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11489\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=16629\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11489\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17028096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11764736\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=74\n",
      "\t\tMap output materialized bytes=67\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=12\n",
      "\t\tCombine output records=5\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=67\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=10\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=241\n",
      "\t\tCPU time spent (ms)=6970\n",
      "\t\tPhysical memory (bytes) snapshot=995160064\n",
      "\t\tVirtual memory (bytes) snapshot=5484478464\n",
      "\t\tTotal committed heap usage (bytes)=849870848\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=110\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=33\n",
      "17/09/25 19:50:02 INFO streaming.StreamJob: Output directory: combiner-output\n"
     ]
    }
   ],
   "source": [
    "# hadoop job w/ 2 reducers but no partitioner class\n",
    "!hdfs dfs -rm -r combiner-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -files combiner.py \\\n",
    "    -mapper /bin/cat \\\n",
    "    -combiner combiner.py \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input Data/test.txt \\\n",
    "    -output combiner-output \\\n",
    "    -numReduceTasks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tD\t12\n",
      "A\tC\t4\n"
     ]
    }
   ],
   "source": [
    "# first partition \n",
    "!hdfs dfs -cat combiner-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tC\t5\n",
      "B\tD\t15\n",
      "B\tC\t11\n"
     ]
    }
   ],
   "source": [
    "# second partition \n",
    "!hdfs dfs -cat combiner-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EEK!__ It looks like the combining worked in the first partition but then in the second there's that extra 'C' record that appears at the end? It makes sense that the combiner won't work if the mapper input is out of order, but I thought my input file was in order? In fact lets check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tC\t1\n",
      "A\tC\t2\n",
      "A\tC\t1\n",
      "A\tD\t5\n",
      "A\tD\t5\n",
      "A\tD\t2\n",
      "B\tC\t5\n",
      "B\tC\t1\n",
      "B\tC\t10\n",
      "B\tD\t2\n",
      "B\tD\t10\n",
      "B\tD\t3"
     ]
    }
   ],
   "source": [
    "# input file\n",
    "!cat test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=combo2></a>\n",
    "So back to the question at hand: \n",
    "\n",
    "__Q:__ _Why doesn't my combiner recognize that the mapper output is sorted?_  \n",
    "\n",
    "__A (take 2):__ Haddop doesn't preserve the order of the records unless you tell it to. So while the input file is sorted and the mapper is just `/bin/cat` when the records leave the mapper & head over to the combiner Hadoop is no longer paying attention to which line was first or second or third. Instead Hadoop has reverted to its default behavior: _records with the same key will be 'shuffled' around together_. \n",
    "\n",
    "As we learned in from observing Hadoop's ([default partitioning](#default)) behavior, if we don't specify `stream.num.map.output.keyfields` then the key is simply the first field. In other words, in our example, the only order Hadoop pays attention to is the distinction between 'A' and 'B'. In fact we can see that the records order get scrambled even without the use of the combiner. Here's another look at the output from the [default partitioning](#default) example at the start of the notebook. (recall that both the mapper & reducer were `/bin/cat` and that the only extra option we specified was 2 reducers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tD\t2\n",
      "A\tD\t5\n",
      "A\tD\t5\n",
      "A\tC\t1\n",
      "A\tC\t2\n",
      "A\tC\t1\n"
     ]
    }
   ],
   "source": [
    "# first partition, the 'D's and 'C' seem to be in order\n",
    "!hdfs dfs -cat no-partitioner-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tC\t5\n",
      "B\tD\t3\n",
      "B\tD\t10\n",
      "B\tD\t2\n",
      "B\tC\t10\n",
      "B\tC\t1\n"
     ]
    }
   ],
   "source": [
    "# second partition -- WHOA, there is a 'C' out of place!\n",
    "!hdfs dfs -cat no-partitioner-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the first partition got lucky, but in general these records are not guaranteed to be sorted on anything except the first field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=combo3></a>\n",
    "## Combining with Composite Keys\n",
    "[Return to Contents](#TOC) | [Skip to My Combiner still doesn't work](#combo-default)  \n",
    "\n",
    "__Q:__ _How can I make sure my combiner DOES receive sorted input?_   \n",
    "__A:__ As with our [partition sort](example) above, we'll need to make sure that Hadoop's shuffle phase pays attention to both the first and 2nd field:\n",
    ">__`-D stream.num.map.output.key.fields=2`__    \n",
    "\n",
    "and though we only want it to _partition on the first field_...  \n",
    "> __`-D mapreduce.partition.keypartitioner.options=\"-k1,1\"`__ \n",
    "__`-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner`__\n",
    "\n",
    "... we want the keys _sorted on both primary and secondary key_:\n",
    ">__`-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator`__\n",
    "__`-D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\"`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted combiner-sort-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob9096172325350348427.jar tmpDir=null\n",
      "17/09/25 19:50:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/25 19:50:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/25 19:50:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/25 19:50:53 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/25 19:50:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506368833436_0002\n",
      "17/09/25 19:50:54 INFO impl.YarnClientImpl: Submitted application application_1506368833436_0002\n",
      "17/09/25 19:50:54 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1506368833436_0002/\n",
      "17/09/25 19:50:54 INFO mapreduce.Job: Running job: job_1506368833436_0002\n",
      "17/09/25 19:51:03 INFO mapreduce.Job: Job job_1506368833436_0002 running in uber mode : false\n",
      "17/09/25 19:51:03 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/25 19:51:13 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/25 19:51:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/25 19:51:23 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/09/25 19:51:25 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/25 19:51:26 INFO mapreduce.Job: Job job_1506368833436_0002 completed successfully\n",
      "17/09/25 19:51:26 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=55\n",
      "\t\tFILE: Number of bytes written=585200\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=33\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15389\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13277\n",
      "\t\tTotal time spent by all map tasks (ms)=15389\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13277\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15389\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13277\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15758336\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=13595648\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=74\n",
      "\t\tMap output materialized bytes=67\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=12\n",
      "\t\tCombine output records=5\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=67\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=10\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=256\n",
      "\t\tCPU time spent (ms)=6270\n",
      "\t\tPhysical memory (bytes) snapshot=988168192\n",
      "\t\tVirtual memory (bytes) snapshot=5495009280\n",
      "\t\tTotal committed heap usage (bytes)=773324800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=110\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=33\n",
      "17/09/25 19:51:26 INFO streaming.StreamJob: Output directory: combiner-sort-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r combiner-sort-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -files combiner.py \\\n",
    "    -mapper /bin/cat \\\n",
    "    -combiner combiner.py \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input Data/test.txt \\\n",
    "    -output combiner-sort-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tC\t11\n",
      "B\tC\t5\n",
      "B\tD\t15\n"
     ]
    }
   ],
   "source": [
    "# first partition\n",
    "!hdfs dfs -cat combiner-sort-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tC\t4\n",
      "A\tD\t12\n"
     ]
    }
   ],
   "source": [
    "# second partition\n",
    "!hdfs dfs -cat combiner-sort-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EEEK!__ The keys now seem to be arriving in order, so why doesn't the combining work?   \n",
    "(_queue suspenseful music_) ... keep reading to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=combo-default></a>\n",
    "## Default Combiner Behavior\n",
    "[Return to Contents](#TOC) | [Skip to Why does my Combiner mess up secondary sort](#funky-stuff)  \n",
    "\n",
    "__Q:__ _In the example above, why didn't the combining work fully?_   \n",
    "__A:__ Again (sigh!) this is expected behavior. Hadoop doesn't guarantee that the combiner will be run on every record... it makes that decision at runtime. Thats why you need to be sure that the mapper output format matches the combiner output format -- your reducer needs to work regardless of whether any combining happens. This is also why, if you want to see fully combined output, you need to use a real reducer (instead of the `/bin/cat` we've been using.) In the final call below, we'll just use the same combiner script as a reducer and you'll see some nice output finally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted combiner-final\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob3694188382917670316.jar tmpDir=null\n",
      "17/09/25 19:51:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/25 19:51:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/25 19:51:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/25 19:51:58 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/25 19:51:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506368833436_0003\n",
      "17/09/25 19:51:59 INFO impl.YarnClientImpl: Submitted application application_1506368833436_0003\n",
      "17/09/25 19:51:59 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1506368833436_0003/\n",
      "17/09/25 19:51:59 INFO mapreduce.Job: Running job: job_1506368833436_0003\n",
      "17/09/25 19:52:08 INFO mapreduce.Job: Job job_1506368833436_0003 running in uber mode : false\n",
      "17/09/25 19:52:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/25 19:52:17 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/25 19:52:18 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/25 19:52:25 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/09/25 19:52:26 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/25 19:52:27 INFO mapreduce.Job: Job job_1506368833436_0003 completed successfully\n",
      "17/09/25 19:52:28 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=55\n",
      "\t\tFILE: Number of bytes written=585172\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=27\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13024\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12445\n",
      "\t\tTotal time spent by all map tasks (ms)=13024\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12445\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13024\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=12445\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13336576\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12743680\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=74\n",
      "\t\tMap output materialized bytes=67\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=12\n",
      "\t\tCombine output records=5\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=67\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=10\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=215\n",
      "\t\tCPU time spent (ms)=5070\n",
      "\t\tPhysical memory (bytes) snapshot=972349440\n",
      "\t\tVirtual memory (bytes) snapshot=5498183680\n",
      "\t\tTotal committed heap usage (bytes)=774373376\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=110\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27\n",
      "17/09/25 19:52:28 INFO streaming.StreamJob: Output directory: combiner-final\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r combiner-final\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -files combiner.py \\\n",
    "    -mapper /bin/cat \\\n",
    "    -combiner combiner.py \\\n",
    "    -reducer combiner.py \\\n",
    "    -input Data/test.txt \\\n",
    "    -output combiner-final \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tC\t16\n",
      "B\tD\t15\n"
     ]
    }
   ],
   "source": [
    "# first partition\n",
    "!hdfs dfs -cat combiner-final/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tC\t4\n",
      "A\tD\t12\n"
     ]
    }
   ],
   "source": [
    "# second partition\n",
    "!hdfs dfs -cat combiner-final/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Wooo hooo!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=funky-stuff></a>\n",
    "## Funky Stuff\n",
    "[Return to Contents](#TOC) | [Skip to More Funky Stuff](#more-funky-stuff)  \n",
    "\n",
    "\n",
    "__Q:__ _Why did the combiner mess up my partial sort?_  \n",
    "__A:__ :) I think you know where this is going by now. That's right. It didn't.\n",
    "\n",
    "As you might expect, there are a ton of funky things that happen when you use different combinations of the parameters we've explored above. I think you should be able to explain them if you think through the following questions carefully:\n",
    "* What fields have I told Hadoop to pay attention to?\n",
    " * (_and does Hadoop know how to recognize these fields -- we haven't explored that here... its the 'delimiter' option though, default is tab_)\n",
    "* What fields did I tell Hadoop to partition on?\n",
    "* What fields did I tell Hadoop sort by?\n",
    "* Have my instructions contradicted each other? If so, what default behavior will Hadoop revert to?\n",
    "* Will the way Hadoop is going to sort produce records ordered in the way my reducer & combiner expect?\n",
    "\n",
    "Using these questions, take a look at the funkiness below ... can you figure out why the answer looks like it does? [Note -- test your understanding by trying to explain the observed output BEFORE you try to fix it.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted funky1-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.1.jar] /tmp/streamjob3827876995430545254.jar tmpDir=null\n",
      "17/09/22 23:57:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/22 23:57:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/09/22 23:57:41 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/09/22 23:57:41 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/09/22 23:57:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506091508167_0067\n",
      "17/09/22 23:57:42 INFO impl.YarnClientImpl: Submitted application application_1506091508167_0067\n",
      "17/09/22 23:57:42 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1506091508167_0067/\n",
      "17/09/22 23:57:42 INFO mapreduce.Job: Running job: job_1506091508167_0067\n",
      "17/09/22 23:57:52 INFO mapreduce.Job: Job job_1506091508167_0067 running in uber mode : false\n",
      "17/09/22 23:57:52 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/09/22 23:58:03 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/09/22 23:58:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/09/22 23:58:11 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/09/22 23:58:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/09/22 23:58:13 INFO mapreduce.Job: Job job_1506091508167_0067 completed successfully\n",
      "17/09/22 23:58:13 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=79\n",
      "\t\tFILE: Number of bytes written=585224\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=324\n",
      "\t\tHDFS: Number of bytes written=45\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12528\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11801\n",
      "\t\tTotal time spent by all map tasks (ms)=12528\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11801\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12528\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11801\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12828672\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12084224\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=86\n",
      "\t\tMap output materialized bytes=91\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=12\n",
      "\t\tCombine output records=8\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=91\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=171\n",
      "\t\tCPU time spent (ms)=5810\n",
      "\t\tPhysical memory (bytes) snapshot=969355264\n",
      "\t\tVirtual memory (bytes) snapshot=5469356032\n",
      "\t\tTotal committed heap usage (bytes)=703594496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=110\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=45\n",
      "17/09/22 23:58:13 INFO streaming.StreamJob: Output directory: funky1-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r funky1-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k3,3nr -k2,2\" \\\n",
    "    -files combiner.py \\\n",
    "    -mapper /bin/cat \\\n",
    "    -combiner combiner.py \\\n",
    "    -reducer combiner.py \\\n",
    "    -input Data/test.txt \\\n",
    "    -output funky1-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\tC\t15\n",
      "B\tD\t15\n",
      "B\tC\t1\n"
     ]
    }
   ],
   "source": [
    "# first partition\n",
    "!hdfs dfs -cat funky1-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tD\t10\n",
      "A\tC\t2\n",
      "A\tD\t2\n",
      "A\tC\t1\n"
     ]
    }
   ],
   "source": [
    "# second partition\n",
    "!hdfs dfs -cat funky1-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=more-funky-stuff></a>\n",
    "## More Funky Stuff\n",
    "\n",
    "[Return to Contents](#TOC) \n",
    "\n",
    "Was that fun? Here are some more funky outputs... feel free to run them & test your understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r funky2-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -mapper /bin/cat \\\n",
    "    -reducer /bin/cat \\\n",
    "    -input Data/test.txt \\\n",
    "    -output funky2-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first partition\n",
    "!hdfs dfs -cat funky2-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second partition\n",
    "!hdfs dfs -cat funky2-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r funky3-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -files combiner.py \\\n",
    "    -mapper /bin/cat \\\n",
    "    -combiner combiner.py\n",
    "    -reducer /bin/cat \\\n",
    "    -input Data/test.txt \\\n",
    "    -output funky3-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first partition\n",
    "!hdfs dfs -cat funky3-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second partition\n",
    "!hdfs dfs -cat funky3-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r funky4-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k2,2\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2 -k1,1\" \\\n",
    "    -files combiner.py \\\n",
    "    -mapper /bin/cat \\\n",
    "    -combiner combiner.py \\\n",
    "    -reducer combiner.py \\\n",
    "    -input Data/test.txt \\\n",
    "    -output funky4-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first partition\n",
    "!hdfs dfs -cat funky4-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second partition\n",
    "!hdfs dfs -cat funky4-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "304px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
